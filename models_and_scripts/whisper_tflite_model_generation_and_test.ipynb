{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdraafat/C-/blob/master/models_and_scripts/whisper_tflite_model_generation_and_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AveoTQ563b3E"
      },
      "source": [
        "##Install TensorFlow, Tranformers and datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3Dhj15CQ3ath",
        "outputId": "bceae06d-16de-49bc-cef1-b3c870cfd7f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow==2.19.0 in /usr/local/lib/python3.11/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.19.0) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.19.0) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.19.0) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.19.0) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.19.0) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.19.0) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.19.0) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.19.0) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.19.0) (4.25.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.19.0) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.19.0) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.19.0) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.19.0) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.19.0) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.19.0) (1.14.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.19.0) (1.71.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.19.0) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.19.0) (3.9.2)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.19.0) (2.1.3)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.19.0) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.19.0) (0.5.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.19.0) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.19.0) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow==2.19.0) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.19.0) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard~=2.19.0->tensorflow==2.19.0) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow==2.19.0) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow==2.19.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow==2.19.0) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.19.0) (0.1.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.1.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2023.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.1.3)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.11/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets) (2023.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (4.13.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow==2.19.0\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install typing-extensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b2AJbu5CJ9Z"
      },
      "source": [
        "## Configure model to be generated as per requirement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUj7pZUWCJUI",
        "outputId": "a5a1a696-573c-4d0e-98d2-9e2b77bf2553"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 50272], [2, 50359], [3, 50363]]\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "\n",
        "######## Set the model as per requirement\n",
        "model_name = \"whisper-tiny\"          # whisper-tiny, whisper-tiny.en, whisper-base, whisper-base.en, whisper-small, whisper-small.en\n",
        "\n",
        "######## Set the language, task, and options as per requirement\n",
        "language_code = \"<|ar|>\"             # <|en|>, <|fr|>, <|hi|>, <|ko|>, <|de|>, <|zh|>, <|ja|>, <|es|>, <|ar|>, <|ru|>, ...\n",
        "task_code     = \"<|transcribe|>\"     # <|transcribe|>, <|translate|>\n",
        "option_code   = \"<|notimestamps|>\"   # <|notimestamps|>, <|nocaptions|>\n",
        "\n",
        "# URL of the JSON file which stores the code mappings\n",
        "url = \"https://huggingface.co/tarteel-ai/whisper-tiny-ar-quran/resolve/main/added_tokens.json\"\n",
        "\n",
        "# Send a GET request to download the file\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    # Parse the JSON content\n",
        "    code_mappings = response.json()\n",
        "else:\n",
        "    print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
        "    code_mappings = {}\n",
        "\n",
        "# Construct forced_decoder_ids using the mappings\n",
        "forced_decoder_ids = [\n",
        "    [1, code_mappings[language_code]],\n",
        "    [2, code_mappings[task_code]],\n",
        "    [3, code_mappings[option_code]]\n",
        "]\n",
        "\n",
        "print(forced_decoder_ids)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade numba\n",
        "\n"
      ],
      "metadata": {
        "id": "O6zY298eDOqT",
        "outputId": "60f358c2-79cd-4e49-e66f-6b2e7840954c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (0.60.0)\n",
            "Collecting numba\n",
            "  Downloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba)\n",
            "  Downloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: numpy<2.3,>=1.24 in /usr/local/lib/python3.11/dist-packages (from numba) (2.2.4)\n",
            "Downloading numba-0.61.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading llvmlite-0.44.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: llvmlite, numba\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.43.0\n",
            "    Uninstalling llvmlite-0.43.0:\n",
            "      Successfully uninstalled llvmlite-0.43.0\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.60.0\n",
            "    Uninstalling numba-0.60.0:\n",
            "      Successfully uninstalled numba-0.60.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "distributed-ucxx-cu12 0.42.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "dask-cuda 25.2.0 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "cuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed llvmlite-0.44.0 numba-0.61.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ng4Qq2rejVOE"
      },
      "source": [
        "##Import the libraries, load the model, do the inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HKfmQ2kp627t",
        "outputId": "cf638600-1bdc-46f1-a787-631a62cdcba4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All PyTorch model weights were used when initializing TFWhisperForConditionalGeneration.\n",
            "\n",
            "All the weights of TFWhisperForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFWhisperForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[50258 50259 50359 50363  2221    13  2326   388   391   307   264 50244\n",
            "    295   264  2808  5359   293   321   366  5404   281  2928   702 14943\n",
            "     13 50257]], shape=(1, 26), dtype=int32)\n",
            "<|startoftranscript|><|en|><|transcribe|><|notimestamps|> Mr. Quilter is the apostle of the middle classes and we are glad to welcome his gospel.<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import transformers\n",
        "import datasets\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import WhisperProcessor, WhisperFeatureExtractor, TFWhisperForConditionalGeneration, WhisperTokenizer\n",
        "\n",
        "pretrained_model = f\"openai/{model_name}\"\n",
        "tflite_model_path = f\"{model_name}.tflite\"\n",
        "saved_model_dir = f\"tf_{model_name}_saved\"\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(pretrained_model)\n",
        "tokenizer = WhisperTokenizer.from_pretrained(pretrained_model, predict_timestamps=True)\n",
        "processor = WhisperProcessor(feature_extractor, tokenizer)\n",
        "model = TFWhisperForConditionalGeneration.from_pretrained(pretrained_model)\n",
        "\n",
        "# Loading dataset\n",
        "ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
        "inputs = feature_extractor(ds[0][\"audio\"][\"array\"], sampling_rate=ds[0][\"audio\"][\"sampling_rate\"], return_tensors=\"tf\")\n",
        "input_features = inputs.input_features\n",
        "\n",
        "# Generating Transcription\n",
        "generated_ids = model.generate(input_features=input_features)\n",
        "print(generated_ids)\n",
        "\n",
        "transcription = processor.tokenizer.decode(generated_ids[0])\n",
        "print(transcription)\n",
        "\n",
        "# Save the model\n",
        "# model.save(saved_model_dir) # not need to save here, saving using tf.saved_model.save() call"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA65NFFIj4bN"
      },
      "source": [
        "## Prompt fix, patch to make forced_decoder_ids work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "p1VOFBO9_MO-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from transformers import TFForceTokensLogitsProcessor, TFLogitsProcessor\n",
        "from typing import List, Optional, Union, Any\n",
        "\n",
        "# Patching methods of class TFForceTokensLogitsProcessor(TFLogitsProcessor):\n",
        "\n",
        "def my__init__(self, force_token_map: List[List[int]]):\n",
        "    force_token_map = dict(force_token_map)\n",
        "    # Converts the dictionary of format {index: token} containing the tokens to be forced to an array, where the\n",
        "    # index of the array corresponds to the index of the token to be forced, for XLA compatibility.\n",
        "    # Indexes without forced tokens will have an negative value.\n",
        "    force_token_array = np.ones((max(force_token_map.keys()) + 1), dtype=np.int32) * -1\n",
        "    for index, token in force_token_map.items():\n",
        "        if token is not None:\n",
        "            force_token_array[index] = token\n",
        "    self.force_token_array = tf.convert_to_tensor(force_token_array, dtype=tf.int32)\n",
        "\n",
        "def my__call__(self, input_ids: tf.Tensor, scores: tf.Tensor, cur_len: int) -> tf.Tensor:\n",
        "    def _force_token(generation_idx):\n",
        "        batch_size = scores.shape[0]\n",
        "        current_token = self.force_token_array[generation_idx]\n",
        "\n",
        "        # Original code below generates NaN values when the model is exported to tflite\n",
        "        # it just needs to be a negative number so that the forced token's value of 0 is the largest\n",
        "        # so it will get chosen\n",
        "        #new_scores = tf.ones_like(scores, dtype=scores.dtype) * -float(\"inf\")\n",
        "        new_scores = tf.ones_like(scores, dtype=scores.dtype) * -float(1)\n",
        "        indices = tf.stack((tf.range(batch_size), tf.tile([current_token], [batch_size])), axis=1)\n",
        "        updates = tf.zeros((batch_size,), dtype=scores.dtype)\n",
        "        new_scores = tf.tensor_scatter_nd_update(new_scores, indices, updates)\n",
        "        return new_scores\n",
        "\n",
        "    scores = tf.cond(\n",
        "        tf.greater_equal(cur_len, tf.shape(self.force_token_array)[0]),\n",
        "        # If the current length is geq than the length of force_token_array, the processor does nothing.\n",
        "        lambda: tf.identity(scores),\n",
        "        # Otherwise, it may force a certain token.\n",
        "        lambda: tf.cond(\n",
        "            tf.greater_equal(self.force_token_array[cur_len], 0),\n",
        "            # Only valid (positive) tokens are forced\n",
        "            lambda: _force_token(cur_len),\n",
        "            # Otherwise, the processor does nothing.\n",
        "            lambda: scores,\n",
        "        ),\n",
        "    )\n",
        "    return scores\n",
        "\n",
        "TFForceTokensLogitsProcessor.__init__ = my__init__\n",
        "TFForceTokensLogitsProcessor.__call__ = my__call__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5c0ZSGWj9Gq"
      },
      "source": [
        "##Define a model with a serving signature and save it in TF SavedModel format.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "K968u9QS5jPL"
      },
      "outputs": [],
      "source": [
        "class GenerateModel(tf.Module):\n",
        "  def __init__(self, model):\n",
        "    super(GenerateModel, self).__init__()\n",
        "    self.model = model\n",
        "\n",
        "  @tf.function(\n",
        "    # shouldn't need static batch size, but throws exception without it (needs to be fixed)\n",
        "    input_signature=[\n",
        "      tf.TensorSpec((1, 80, 3000), tf.float32, name=\"input_features\"),\n",
        "    ],\n",
        "  )\n",
        "  def serving(self, input_features):\n",
        "    outputs = self.model.generate(\n",
        "      input_features,\n",
        "      # change below if you think your output will be bigger\n",
        "      # aka if you have bigger transcriptions\n",
        "      # you can make it 200 for example\n",
        "      max_new_tokens=448,\n",
        "      return_dict_in_generate=True,\n",
        "      forced_decoder_ids=forced_decoder_ids,\n",
        "    )\n",
        "    return {\"sequences\": outputs[\"sequences\"]}\n",
        "\n",
        "generate_model = GenerateModel(model=model)\n",
        "tf.saved_model.save(generate_model, saved_model_dir, signatures={\"serving_default\": generate_model.serving})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLfY_Fv4qHSC"
      },
      "source": [
        "## Convert the model from TF SavedModel format to TF lite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "LBD7wMdQnsYc",
        "outputId": "e35090bc-ec4c-4ae6-ef78-c32a9b37008f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ff85071f-2953-49f2-b6da-d3f735e79943\", \"model.tflite\", 40591072)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Convert the model\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
        "converter.target_spec.supported_ops = [\n",
        "  tf.lite.OpsSet.TFLITE_BUILTINS, # enable TensorFlow Lite ops.\n",
        "  tf.lite.OpsSet.SELECT_TF_OPS # enable TensorFlow ops.\n",
        "]\n",
        "\n",
        "# Learn about post training quantization\n",
        "# https://www.tensorflow.org/lite/performance/post_training_quantization\n",
        "\n",
        "# Dynamic range quantization which reduces the size of the model to 25%\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Float16 quantization reduces the size to 50%\n",
        "# converter.target_spec.supported_types = [tf.float16]\n",
        "\n",
        "tflite_model = converter.convert()\n",
        "from google.colab import files\n",
        "\n",
        "# Save the model to a file\n",
        "with open(\"model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "# Download it to your local device\n",
        "files.download(\"model.tflite\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jRRP3nQkEtC"
      },
      "source": [
        "##Test tflite model using TFLite Interpreter. Check transcription for dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3fbeAWE5ikD"
      },
      "outputs": [],
      "source": [
        "# loaded model... now with generate!\n",
        "interpreter = tf.lite.Interpreter(tflite_model_path)\n",
        "\n",
        "tflite_generate = interpreter.get_signature_runner()\n",
        "generated_ids = tflite_generate(input_features=input_features)[\"sequences\"]\n",
        "# print(generated_ids)\n",
        "\n",
        "transcription = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "print(transcription)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOiytfUd4yyp"
      },
      "source": [
        "## Install faster-whisper for audio processing and testing model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-k7BGLqM5Egh"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/SYSTRAN/faster-whisper.git\n",
        "!pip install faster-whisper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xO1ZerhS4-in"
      },
      "source": [
        "## Test all audio files in loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Oc3YEK2j1FLb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "from transformers import WhisperProcessor, WhisperFeatureExtractor\n",
        "from faster_whisper import decode_audio\n",
        "\n",
        "# Set up paths and model (whisper-tiny, whisper-tiny.en, whisper-base, whisper-base.en, whisper-small, whisper-small.en)\n",
        "# model_name = \"whisper-base.en\"\n",
        "# pretrained_model = f\"openai/{model_name}\"\n",
        "# tflite_model_path = f\"{model_name}.tflite\"\n",
        "\n",
        "######## NOTE: Specify the folder containing audio files\n",
        "!git clone https://github.com/vilassn/audio_samples.git\n",
        "audio_folder_path = 'audio_samples/en'\n",
        "#audio_folder_path = '/content/drive/MyDrive/Colab Notebooks/audio'\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(pretrained_model)\n",
        "tokenizer = WhisperTokenizer.from_pretrained(pretrained_model, predict_timestamps=True)\n",
        "processor = WhisperProcessor(feature_extractor, tokenizer)\n",
        "\n",
        "interpreter = tf.lite.Interpreter(tflite_model_path)\n",
        "tflite_generate = interpreter.get_signature_runner()\n",
        "\n",
        "# Number of iterations you want the loop to run\n",
        "iterations = 1000\n",
        "\n",
        "for i in range(1, iterations + 1):  # Start from 1 to print iteration number\n",
        "    print(f\"Iteration {i}.......................................................\\n\")  # Print iteration number and newline\n",
        "\n",
        "    # Loop through all files in the folder\n",
        "    for audio_file_name in os.listdir(audio_folder_path):\n",
        "        audio_file_path = os.path.join(audio_folder_path, audio_file_name)\n",
        "\n",
        "        if audio_file_name.endswith('.wav'):  # Process only .wav files\n",
        "            print(f\"Processing {audio_file_name}...\")\n",
        "\n",
        "            # Preprocess the audio file\n",
        "            input_audio = decode_audio(audio_file_path, sampling_rate=16000)\n",
        "            input_features = feature_extractor(input_audio, sampling_rate=16000, return_tensors=\"tf\").input_features\n",
        "\n",
        "            # Run the model\n",
        "            generated_ids = tflite_generate(input_features=input_features)[\"sequences\"]\n",
        "\n",
        "            # Decode and print transcription\n",
        "            transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "            print(f\"{transcription}\\n\")  # Add newline after each transcription"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}